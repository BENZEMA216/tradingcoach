"""
ä»»åŠ¡ç®¡ç†å™¨

input: Task æ¨¡å‹, é…ç½®, EventDetector
output: ä»»åŠ¡åˆ›å»ºã€æ‰§è¡Œã€çŠ¶æ€è¿½è¸ª
pos: åç«¯æœåŠ¡å±‚ - ç®¡ç†å¼‚æ­¥åˆ†æä»»åŠ¡çš„æ‰§è¡Œ

åŠŸèƒ½:
- å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ (ThreadPoolExecutor)
- è¯¦ç»†å¤„ç†æ—¥å¿— (æ¯æ¡äº¤æ˜“/æŒä»“/è¯„åˆ†/äº‹ä»¶)
- è¿›åº¦è¿½è¸ª (0-100%)
- äº‹ä»¶æ£€æµ‹ (è´¢æŠ¥/ä»·æ ¼å¼‚å¸¸/æˆäº¤é‡å¼‚å¸¸)
- å®Œæˆé€šçŸ¥ (é‚®ä»¶)

ä¸€æ—¦æˆ‘è¢«æ›´æ–°ï¼ŒåŠ¡å¿…æ›´æ–°æˆ‘çš„å¼€å¤´æ³¨é‡Šï¼Œä»¥åŠæ‰€å±æ–‡ä»¶å¤¹çš„README.md
"""

import sys
import uuid
import logging
import tempfile
import traceback
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable, List
from concurrent.futures import ThreadPoolExecutor
import threading

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import config
from src.models.base import init_database, get_session, create_all_tables
from src.models.task import Task, TaskStatus, TaskType

logger = logging.getLogger(__name__)

# æ—¥å¿—é…ç½®
MAX_LOGS = 1000  # æœ€å¤§æ—¥å¿—æ¡æ•°ï¼ˆå¢åŠ ä»¥å®¹çº³æ›´å¤šæ—¥å¿—ï¼‰
LOG_BATCH_SIZE = 1  # æ¯æ¡éƒ½è®°å½•ï¼Œè®©æ—¥å¿—æ›´é¢‘ç¹
import time  # ç”¨äºæ·»åŠ å°å»¶è¿Ÿè®©åŠ¨ç”»æ›´æ˜æ˜¾


class TaskManager:
    """
    ä»»åŠ¡ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼Œä»»åŠ¡çŠ¶æ€å­˜å‚¨åœ¨ SQLite ä¸­
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        # è·å–é…ç½®
        max_workers = getattr(config, 'TASK_MAX_CONCURRENT', 3)

        self._executor = ThreadPoolExecutor(max_workers=max_workers)
        self._tasks = {}  # task_id -> Future
        self._initialized = True

        logger.info(f"TaskManager initialized with {max_workers} workers")

    def create_task(
        self,
        file_name: str,
        file_hash: str,
        file_size: int,
        file_path: str,
        email: Optional[str] = None,
        replace_mode: bool = True
    ) -> str:
        """
        åˆ›å»ºæ–°ä»»åŠ¡

        Args:
            file_name: æ–‡ä»¶å
            file_hash: æ–‡ä»¶å“ˆå¸Œ
            file_size: æ–‡ä»¶å¤§å°
            file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            email: é€šçŸ¥é‚®ç®±ï¼ˆå¯é€‰ï¼‰
            replace_mode: æ˜¯å¦æ›¿æ¢ç°æœ‰æ•°æ®

        Returns:
            task_id: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())[:8]  # ä½¿ç”¨çŸ­UUID

        # åˆå§‹åŒ–æ•°æ®åº“
        init_database(config.DATABASE_URL, echo=False)
        create_all_tables()

        session = get_session()
        try:
            task = Task(
                task_id=task_id,
                task_type=TaskType.CSV_ANALYSIS,
                status=TaskStatus.PENDING,
                progress=0.0,
                current_step="ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¤„ç†",
                file_name=file_name,
                file_hash=file_hash,
                file_size=file_size,
                email=email,
                logs=[]
            )
            task.add_log("ä»»åŠ¡å·²åˆ›å»º")

            session.add(task)
            session.commit()

            logger.info(f"Task created: {task_id}")

        finally:
            session.close()

        # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        future = self._executor.submit(
            self._run_csv_analysis,
            task_id,
            file_path,
            replace_mode
        )
        self._tasks[task_id] = future

        return task_id

    def get_task(self, task_id: str) -> Optional[dict]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if task:
                return task.to_dict()
            return None
        finally:
            session.close()

    def cancel_task(self, task_id: str) -> bool:
        """
        å–æ¶ˆä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        if task_id in self._tasks:
            future = self._tasks[task_id]
            if future.cancel():
                self._update_task_status(task_id, TaskStatus.CANCELLED)
                logger.info(f"Task cancelled: {task_id}")
                return True

        return False

    def _update_task_status(
        self,
        task_id: str,
        status: str,
        progress: float = None,
        step: str = None,
        result: dict = None,
        error: str = None
    ):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if not task:
                logger.error(f"Task not found: {task_id}")
                return

            task.status = status

            if progress is not None:
                task.progress = progress

            if step:
                task.current_step = step
                task.add_log(step)

            if result:
                task.result = result

            if error:
                task.error_message = error
                task.add_log(error, level="error")

            if status == TaskStatus.RUNNING and not task.started_at:
                task.started_at = datetime.utcnow()

            if status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                task.completed_at = datetime.utcnow()

            session.commit()

        except Exception as e:
            logger.error(f"Failed to update task {task_id}: {e}")
            session.rollback()
        finally:
            session.close()

    # æ—¥å¿—å†™å…¥é”ï¼ˆSQLite ä¸æ”¯æŒå¹¶å‘å†™å…¥ï¼‰
    _log_lock = threading.Lock()

    def _add_log(
        self,
        task_id: str,
        message: str,
        level: str = "info",
        category: str = None
    ):
        """
        æ·»åŠ æ—¥å¿—æ¡ç›®ï¼ˆä¸æ›´æ–°è¿›åº¦ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ« (info/success/warning/error)
            category: æ—¥å¿—åˆ†ç±» (import/match/score/system)
        """
        with self._log_lock:  # ä¸²è¡ŒåŒ–æ•°æ®åº“å†™å…¥
            init_database(config.DATABASE_URL, echo=False)
            session = get_session()

            try:
                task = session.query(Task).filter(Task.task_id == task_id).first()
                if not task:
                    return

                log_entry = {
                    "time": datetime.utcnow().isoformat(),
                    "level": level,
                    "message": message,
                }
                if category:
                    log_entry["category"] = category

                if task.logs is None:
                    task.logs = []

                # é™åˆ¶æ—¥å¿—æ•°é‡
                if len(task.logs) >= MAX_LOGS:
                    task.logs = task.logs[-(MAX_LOGS - 1):]

                task.logs = task.logs + [log_entry]  # åˆ›å»ºæ–°åˆ—è¡¨è§¦å‘SQLAlchemyæ›´æ–°
                session.commit()

            except Exception as e:
                logger.error(f"Failed to add log for task {task_id}: {e}")
                session.rollback()
            finally:
                session.close()

    def _run_csv_analysis(self, task_id: str, file_path: str, replace_mode: bool):
        """
        æ‰§è¡Œ CSV åˆ†æä»»åŠ¡ï¼ˆå¸¦è¯¦ç»†æ—¥å¿—ï¼‰

        åˆ†ä¸º4ä¸ªé˜¶æ®µï¼š
        1. æ•°æ®å¯¼å…¥ (0-40%)
        2. FIFOé…å¯¹ (40-70%)
        3. è´¨é‡è¯„åˆ† (70-95%)
        4. å®Œæˆ (95-100%)
        """
        logger.info(f"Starting CSV analysis task: {task_id}")

        self._update_task_status(
            task_id,
            TaskStatus.RUNNING,
            progress=5.0,
            step="å¼€å§‹å¤„ç†æ–‡ä»¶..."
        )
        self._add_log(task_id, "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ", "info", "system")

        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from src.importers.english_csv_parser import detect_csv_language
            from src.importers.incremental_importer import IncrementalImporter
            from src.matchers.fifo_matcher import FIFOMatcher
            from src.analyzers.quality_scorer import QualityScorer
            from src.models.trade import Trade
            from src.models.position import Position
            from sqlalchemy import text

            # ==================== é˜¶æ®µ 1: æ•°æ®å¯¼å…¥ (0-40%) ====================
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=10.0,
                step="æ­£åœ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼..."
            )
            self._add_log(task_id, "æ­£åœ¨æ£€æµ‹ CSV æ–‡ä»¶æ ¼å¼...", "info", "import")

            # æ£€æµ‹è¯­è¨€ - æ·»åŠ æ›´å¤šæ­¥éª¤æ—¥å¿—
            self._add_log(task_id, "è¯»å–æ–‡ä»¶å¤´ä¿¡æ¯...", "info", "import")
            time.sleep(0.1)
            self._add_log(task_id, "åˆ†æå­—æ®µåç§°...", "info", "import")
            time.sleep(0.1)

            language = detect_csv_language(file_path)
            logger.info(f"[{task_id}] Detected language: {language}")

            if language == 'unknown':
                self._add_log(task_id, "æ— æ³•è¯†åˆ«çš„æ–‡ä»¶æ ¼å¼", "error", "import")
                raise ValueError("ä¸æ”¯æŒçš„CSVæ ¼å¼ï¼Œè¯·ä½¿ç”¨å¯Œé€”è¯åˆ¸å¯¼å‡ºçš„CSVæ–‡ä»¶")

            # æ ¼å¼è¯†åˆ«æˆåŠŸ
            format_name = "å¯Œé€”è‹±æ–‡æ ¼å¼" if language == 'english' else "å¯Œé€”ä¸­æ–‡æ ¼å¼"
            self._add_log(task_id, f"âœ“ æ£€æµ‹åˆ°æ ¼å¼: {format_name}", "success", "import")
            time.sleep(0.05)

            # æ›¿æ¢æ¨¡å¼ï¼šå…ˆæ¸…é™¤æ‰€æœ‰æ—§æ•°æ®
            if replace_mode:
                self._update_task_status(
                    task_id,
                    TaskStatus.RUNNING,
                    progress=15.0,
                    step="æ¸…é™¤æ—§æ•°æ®..."
                )
                self._add_log(task_id, "æ­£åœ¨æ¸…é™¤æ—§æ•°æ®...", "info", "import")
                self._add_log(task_id, "æ¸…é™¤äº¤æ˜“è®°å½•è¡¨...", "info", "import")
                time.sleep(0.05)
                self._add_log(task_id, "æ¸…é™¤æŒä»“è®°å½•è¡¨...", "info", "import")
                time.sleep(0.05)
                self._clear_all_trading_data()
                self._add_log(task_id, "æ¸…é™¤å¯¼å…¥å†å²è¡¨...", "info", "import")
                time.sleep(0.05)
                self._add_log(task_id, "âœ“ æ—§æ•°æ®å·²æ¸…é™¤", "success", "import")

            # æ‰§è¡Œå¯¼å…¥
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=20.0,
                step="æ­£åœ¨å¯¼å…¥äº¤æ˜“æ•°æ®..."
            )
            self._add_log(task_id, "å¼€å§‹è§£æ CSV æ–‡ä»¶...", "info", "import")
            self._add_log(task_id, "åˆå§‹åŒ–è§£æå™¨...", "info", "import")
            time.sleep(0.05)
            self._add_log(task_id, "è¯»å–æ–‡ä»¶å†…å®¹...", "info", "import")
            time.sleep(0.05)

            importer = IncrementalImporter(file_path, dry_run=False)

            self._add_log(task_id, "è§£æ CSV åˆ—ç»“æ„...", "info", "import")
            time.sleep(0.05)
            self._add_log(task_id, "æ˜ å°„å­—æ®µåˆ°æ ‡å‡†æ ¼å¼...", "info", "import")
            time.sleep(0.05)

            import_result = importer.run()

            # æ·»åŠ å¯¼å…¥è¯¦æƒ…æ—¥å¿— - æ›´è¯¦ç»†
            self._add_log(task_id, f"æ–‡ä»¶è§£æå®Œæˆ: å…± {import_result.total_rows} è¡Œæ•°æ®", "info", "import")
            time.sleep(0.03)
            self._add_log(task_id, f"å·²æˆäº¤è®¢å•: {import_result.completed_trades} ç¬”", "info", "import")
            time.sleep(0.03)

            if import_result.duplicates_skipped > 0:
                self._add_log(task_id, f"âš  è·³è¿‡é‡å¤è®°å½•: {import_result.duplicates_skipped} ç¬”", "warning", "import")

            if import_result.errors > 0:
                self._add_log(task_id, f"âš  è§£æé”™è¯¯: {import_result.errors} ç¬”", "warning", "import")
                for err_msg in import_result.error_messages[:5]:
                    self._add_log(task_id, f"  â”” {err_msg}", "warning", "import")
                    time.sleep(0.02)

            # æ·»åŠ å¯¼å…¥çš„æ¯æ¡äº¤æ˜“è¯¦æƒ… - å…¨éƒ¨è®°å½•ï¼
            self._log_all_imported_trades(task_id)

            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=40.0,
                step=f"å·²å¯¼å…¥ {import_result.new_trades} æ¡äº¤æ˜“è®°å½•"
            )
            self._add_log(
                task_id,
                f"âœ“ å¯¼å…¥å®Œæˆ: {import_result.new_trades} æ¡æ–°äº¤æ˜“å·²å…¥åº“",
                "success",
                "import"
            )

            # ==================== é˜¶æ®µ 2: FIFOé…å¯¹ (40-70%) ====================
            positions_matched = 0
            positions_scored = 0

            if import_result.new_trades > 0:
                self._update_task_status(
                    task_id,
                    TaskStatus.RUNNING,
                    progress=45.0,
                    step="æ­£åœ¨è¿›è¡ŒæŒä»“é…å¯¹..."
                )
                self._add_log(task_id, "å¼€å§‹ FIFO æŒä»“é…å¯¹ç®—æ³•...", "info", "match")
                time.sleep(0.05)
                self._add_log(task_id, "åˆå§‹åŒ–é…å¯¹å¼•æ“...", "info", "match")
                time.sleep(0.05)
                self._add_log(task_id, "æŒ‰æ ‡çš„åˆ†ç»„äº¤æ˜“è®°å½•...", "info", "match")
                time.sleep(0.05)

                init_database(config.DATABASE_URL, echo=False)
                session = get_session()

                try:
                    matcher = FIFOMatcher(session)

                    self._add_log(task_id, "æ’åºä¹°å…¥/å–å‡ºé˜Ÿåˆ—...", "info", "match")
                    time.sleep(0.05)

                    match_result = matcher.match_all_trades()
                    positions_matched = match_result.get('positions_created', 0)
                    open_positions = match_result.get('open_positions', 0)
                    closed_positions = match_result.get('closed_positions', 0)

                    # æ·»åŠ é…å¯¹è¯¦æƒ…æ—¥å¿—
                    self._add_log(task_id, f"å¤„ç†äº¤æ˜“æ•°: {match_result.get('total_trades', 0)} ç¬”", "info", "match")
                    time.sleep(0.03)
                    self._add_log(task_id, f"æ¶‰åŠæ ‡çš„æ•°: {match_result.get('symbols_processed', 0)} ä¸ª", "info", "match")
                    time.sleep(0.03)

                    # è®°å½•æ¯ä¸ªæŒä»“çš„é…å¯¹ç»“æœ - å…¨éƒ¨ï¼
                    self._log_all_matched_positions(task_id, session)

                    self._add_log(
                        task_id,
                        f"âœ“ é…å¯¹å®Œæˆ: {positions_matched} ä¸ªæŒä»“ (å·²å¹³ä»“: {closed_positions}, æœªå¹³ä»“: {open_positions})",
                        "success",
                        "match"
                    )

                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=70.0,
                        step=f"å·²é…å¯¹ {positions_matched} ä¸ªæŒä»“"
                    )

                    # ==================== é˜¶æ®µ 3: å¸‚åœºæ•°æ®è·å– (70-82%) ====================
                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=70.0,
                        step="æ­£åœ¨è·å–å¸‚åœºæ•°æ®..."
                    )
                    self._add_log(task_id, "å¼€å§‹è·å–å¸‚åœºæ•°æ®...", "info", "data")

                    # è·å–è‚¡ç¥¨æ•°æ®
                    market_data_result = self._fetch_market_data_with_logs(task_id, session)

                    # æ ¹æ®ç»“æœæ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                    symbols_fetched = market_data_result.get('symbols_fetched', 0)
                    symbols_analyzed = market_data_result.get('symbols_analyzed', 0)
                    failed_symbols = market_data_result.get('failed_symbols', [])

                    if symbols_fetched == 0 and symbols_analyzed > 0:
                        # å®Œå…¨å¤±è´¥
                        self._add_log(
                            task_id,
                            f"âš  å¸‚åœºæ•°æ®è·å–å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ‰é™æ•°æ®å®Œæˆè¯„åˆ†",
                            "warning",
                            "data"
                        )
                    elif len(failed_symbols) > 0:
                        # éƒ¨åˆ†å¤±è´¥
                        failed_count = len(failed_symbols)
                        self._add_log(
                            task_id,
                            f"âš  éƒ¨åˆ†æ ‡çš„è·å–å¤±è´¥: {failed_count} ä¸ª (å·²æˆåŠŸ {symbols_fetched} ä¸ª)",
                            "warning",
                            "data"
                        )
                    else:
                        # å…¨éƒ¨æˆåŠŸ
                        self._add_log(
                            task_id,
                            f"âœ“ å¸‚åœºæ•°æ®è·å–å®Œæˆ: {symbols_fetched} ä¸ªæ ‡çš„",
                            "success",
                            "data"
                        )

                    # ==================== é˜¶æ®µ 4: è´¨é‡è¯„åˆ† (82-95%) ====================
                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=85.0,
                        step="æ­£åœ¨è®¡ç®—è´¨é‡è¯„åˆ†..."
                    )
                    self._add_log(task_id, "å¼€å§‹è®¡ç®—è´¨é‡è¯„åˆ† (V2 ä¹ç»´åº¦)...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "åˆå§‹åŒ–è¯„åˆ†å¼•æ“...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "åŠ è½½è¯„åˆ†è§„åˆ™é…ç½®...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "è¯„åˆ†ç»´åº¦: æŠ€æœ¯/è¡Œä¸º/é£æ§/æ‰§è¡Œ/å¸‚åœºç¯å¢ƒ...", "info", "score")
                    time.sleep(0.05)

                    scorer = QualityScorer()
                    score_result = scorer.score_all_positions(session, update_db=True)
                    positions_scored = score_result.get('scored', 0)

                    # è®°å½•æ¯ä¸ªæŒä»“çš„è¯„åˆ† - å…¨éƒ¨ï¼
                    self._log_all_scored_positions(task_id, session)

                    session.commit()

                    self._add_log(
                        task_id,
                        f"âœ“ è¯„åˆ†å®Œæˆ: {positions_scored} ä¸ªæŒä»“å·²è¯„åˆ†",
                        "success",
                        "score"
                    )

                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=90.0,
                        step=f"å·²è¯„åˆ† {positions_scored} ä¸ªæŒä»“"
                    )

                    # ==================== é˜¶æ®µ 4.5: äº‹ä»¶æ£€æµ‹ (90-95%) ====================
                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=90.0,
                        step="æ­£åœ¨æ£€æµ‹å¸‚åœºäº‹ä»¶..."
                    )
                    self._add_log(task_id, "å¼€å§‹æ£€æµ‹å¸‚åœºäº‹ä»¶ (è´¢æŠ¥/ä»·æ ¼å¼‚å¸¸/æˆäº¤é‡å¼‚å¸¸)...", "info", "events")
                    time.sleep(0.05)

                    events_detected = self._detect_events_with_logs(task_id, session)

                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=95.0,
                        step=f"å·²æ£€æµ‹ {events_detected} ä¸ªäº‹ä»¶"
                    )

                except Exception as e:
                    logger.error(f"[{task_id}] Matching/scoring error: {e}")
                    self._add_log(task_id, f"å¤„ç†é”™è¯¯: {str(e)}", "error", "system")
                    session.rollback()
                    raise

                finally:
                    session.close()

            else:
                self._add_log(task_id, "æ— æ–°äº¤æ˜“ï¼Œè·³è¿‡é…å¯¹å’Œè¯„åˆ†", "info", "system")

            # ==================== é˜¶æ®µ 5: å®Œæˆ (100%) ====================
            # å¦‚æœæ²¡æœ‰è¿è¡Œmarket_data_resultï¼Œåˆå§‹åŒ–ä¸ºç©º
            if 'market_data_result' not in locals():
                market_data_result = {'symbols_fetched': 0, 'records_fetched': 0}
            if 'events_detected' not in locals():
                events_detected = 0

            result = {
                "language": language,
                "total_rows": import_result.total_rows,
                "completed_trades": import_result.completed_trades,
                "new_trades": import_result.new_trades,
                "duplicates_skipped": import_result.duplicates_skipped,
                "positions_matched": positions_matched,
                "positions_scored": positions_scored,
                "events_detected": events_detected,
                "symbols_fetched": market_data_result.get('symbols_fetched', 0),
                "market_data_records": market_data_result.get('records_fetched', 0),
                "errors": import_result.errors,
                "error_messages": import_result.error_messages[:10] if import_result.error_messages else [],
                "broker_name": getattr(import_result, 'broker_name', format_name),
            }

            self._add_log(task_id, "åˆ†æå®Œæˆï¼", "success", "system")
            self._update_task_status(
                task_id,
                TaskStatus.COMPLETED,
                progress=100.0,
                step="åˆ†æå®Œæˆï¼",
                result=result
            )

            logger.info(f"[{task_id}] Task completed successfully")

            # å‘é€é‚®ä»¶é€šçŸ¥
            self._send_completion_email(task_id, result)

        except Exception as e:
            logger.error(f"[{task_id}] Task failed: {e}", exc_info=True)
            self._add_log(task_id, f"ä»»åŠ¡å¤±è´¥: {str(e)}", "error", "system")
            self._update_task_status(
                task_id,
                TaskStatus.FAILED,
                error=str(e)
            )

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(file_path).unlink()
            except:
                pass

    def _log_all_imported_trades(self, task_id: str):
        """è®°å½•æ‰€æœ‰å¯¼å…¥çš„äº¤æ˜“ - æ¯æ¡éƒ½è®°å½•ï¼"""
        from src.models.trade import Trade

        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            # è·å–æ‰€æœ‰äº¤æ˜“ï¼ˆæŒ‰æ—¥æœŸæ’åºï¼‰
            trades = session.query(Trade).order_by(Trade.trade_date.asc()).all()

            if not trades:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(trades)} æ¡äº¤æ˜“æ˜ç»†...", "info", "import")
            time.sleep(0.03)

            for i, trade in enumerate(trades, 1):
                direction = "ä¹°å…¥" if trade.direction == "BUY" else "å–å‡º"
                direction_icon = "ğŸ“ˆ" if trade.direction == "BUY" else "ğŸ“‰"

                # æ ¼å¼åŒ–ä»·æ ¼
                price_str = f"${trade.price:.2f}" if trade.price else "N/A"

                self._add_log(
                    task_id,
                    f"{direction_icon} #{i} {trade.symbol} {direction} {trade.quantity}è‚¡ @ {price_str}",
                    "info",
                    "import"
                )

                # æ¯5æ¡åŠ ä¸€ç‚¹å»¶è¿Ÿï¼Œè®©åŠ¨ç”»æ›´æ˜æ˜¾
                if i % 5 == 0:
                    time.sleep(0.02)

            self._add_log(task_id, f"âœ“ å·²è®°å½• {len(trades)} æ¡äº¤æ˜“æ˜ç»†", "success", "import")

        except Exception as e:
            logger.warning(f"Failed to log trades: {e}")
        finally:
            session.close()

    def _log_all_matched_positions(self, task_id: str, session):
        """è®°å½•æ‰€æœ‰é…å¯¹çš„æŒä»“ - æ¯ä¸ªéƒ½è®°å½•ï¼"""
        from src.models.position import Position

        try:
            # è·å–æ‰€æœ‰æŒä»“
            positions = session.query(Position).order_by(Position.open_date.asc()).all()

            if not positions:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(positions)} ä¸ªæŒä»“é…å¯¹ç»“æœ...", "info", "match")
            time.sleep(0.03)

            for i, pos in enumerate(positions, 1):
                direction = "å¤šå¤´" if pos.direction == "LONG" else "ç©ºå¤´"
                direction_icon = "ğŸ”º" if pos.direction == "LONG" else "ğŸ”»"
                status = "å·²å¹³ä»“" if pos.status == "CLOSED" else "æŒä»“ä¸­"
                status_icon = "âœ…" if pos.status == "CLOSED" else "â³"

                # ç›ˆäºä¿¡æ¯
                if pos.realized_pnl is not None and pos.realized_pnl != 0:
                    pnl_icon = "ğŸ’°" if pos.realized_pnl > 0 else "ğŸ’¸"
                    pnl_str = f"{pnl_icon} ${pos.realized_pnl:+,.2f}"
                    level = "success" if pos.realized_pnl > 0 else "warning"
                else:
                    pnl_str = ""
                    level = "info"

                self._add_log(
                    task_id,
                    f"{direction_icon} #{i} {pos.symbol} {direction} {pos.quantity}è‚¡ {status_icon}{status} {pnl_str}",
                    level,
                    "match"
                )

                # æ¯3ä¸ªæŒä»“åŠ ä¸€ç‚¹å»¶è¿Ÿ
                if i % 3 == 0:
                    time.sleep(0.02)

            self._add_log(task_id, f"âœ“ å·²è®°å½• {len(positions)} ä¸ªæŒä»“", "success", "match")

        except Exception as e:
            logger.warning(f"Failed to log positions: {e}")

    def _log_all_scored_positions(self, task_id: str, session):
        """è®°å½•æ‰€æœ‰æŒä»“çš„è¯„åˆ† - æ¯ä¸ªéƒ½è®°å½•ï¼"""
        from src.models.position import Position

        try:
            # è·å–æ‰€æœ‰å·²è¯„åˆ†çš„æŒä»“
            positions = session.query(Position).filter(
                Position.overall_score.isnot(None)
            ).order_by(Position.overall_score.desc()).all()

            if not positions:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(positions)} ä¸ªæŒä»“è¯„åˆ†...", "info", "score")
            time.sleep(0.03)

            for i, pos in enumerate(positions, 1):
                grade = pos.score_grade or "?"
                score = pos.overall_score or 0

                # æ ¹æ®ç­‰çº§é€‰æ‹©å›¾æ ‡
                grade_icons = {
                    'A': 'ğŸ†',
                    'B': 'â­',
                    'C': 'ğŸ“Š',
                    'D': 'âš ï¸',
                    'F': 'âŒ'
                }
                icon = grade_icons.get(grade, 'ğŸ“‹')

                # æ ¹æ®ç­‰çº§é€‰æ‹©æ—¥å¿—çº§åˆ«
                if grade in ['A', 'B']:
                    level = "success"
                elif grade in ['D', 'F']:
                    level = "warning"
                else:
                    level = "info"

                self._add_log(
                    task_id,
                    f"{icon} #{i} {pos.symbol} â†’ {grade}çº§ (åˆ†æ•°: {score:.1f})",
                    level,
                    "score"
                )

                # æ¯3ä¸ªè¯„åˆ†åŠ ä¸€ç‚¹å»¶è¿Ÿ
                if i % 3 == 0:
                    time.sleep(0.02)

            # ç»Ÿè®¡å„ç­‰çº§æ•°é‡
            grade_counts = {}
            for pos in positions:
                g = pos.score_grade or "?"
                grade_counts[g] = grade_counts.get(g, 0) + 1

            grade_summary = " | ".join([f"{g}çº§:{c}ä¸ª" for g, c in sorted(grade_counts.items())])
            self._add_log(task_id, f"ğŸ“Š è¯„åˆ†åˆ†å¸ƒ: {grade_summary}", "info", "score")

        except Exception as e:
            logger.warning(f"Failed to log scores: {e}")

    def _fetch_market_data_with_logs(self, task_id: str, session) -> dict:
        """
        è·å–å¸‚åœºæ•°æ®å¹¶è®°å½•æ—¥å¿—ï¼ˆé€ symbol æ—¥å¿—ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            session: æ•°æ®åº“ä¼šè¯

        Returns:
            dict with fetch statistics
        """
        from src.data_sources.batch_fetcher import BatchFetcher
        from src.data_sources.cache_manager import CacheManager
        from src.models.trade import Trade
        from sqlalchemy import func

        try:
            self._add_log(task_id, "åˆå§‹åŒ–æ•°æ®è·å–å¼•æ“...", "info", "data")
            time.sleep(0.05)

            # è·å–æ‰€æœ‰äº¤æ˜“æ ‡çš„
            symbols = session.query(Trade.symbol).distinct().all()
            symbol_list = [s[0] for s in symbols]

            if not symbol_list:
                self._add_log(task_id, "âš  æ²¡æœ‰æ‰¾åˆ°éœ€è¦è·å–æ•°æ®çš„æ ‡çš„", "warning", "data")
                return {'symbols_fetched': 0, 'records_fetched': 0}

            self._add_log(task_id, f"å‘ç° {len(symbol_list)} ä¸ªéœ€è¦è·å–æ•°æ®çš„æ ‡çš„", "info", "data")
            time.sleep(0.03)

            # æ˜¾ç¤ºéƒ¨åˆ†æ ‡çš„
            sample_symbols = symbol_list[:5]
            self._add_log(task_id, f"æ ‡çš„é¢„è§ˆ: {', '.join(sample_symbols)}...", "info", "data")
            time.sleep(0.03)

            # åˆå§‹åŒ– BatchFetcher
            self._add_log(task_id, "åˆå§‹åŒ–æ•°æ®è·¯ç”±å™¨ (YFinance + AKShare)...", "info", "data")
            time.sleep(0.05)

            cache_manager = CacheManager(db_session=session)
            fetcher = BatchFetcher(
                cache_manager=cache_manager,
                use_router=True,
                max_workers=1,  # ä¸²è¡Œæ¨¡å¼é¿å…çº¿ç¨‹æŒ‚èµ·é—®é¢˜
                request_delay=0.1
            )

            self._add_log(task_id, "å¼€å§‹æ‰¹é‡è·å–å¸‚åœºæ•°æ®...", "info", "data")
            time.sleep(0.05)

            # æ›´æ–°è¿›åº¦
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=72.0,
                step="æ­£åœ¨è·å–å¸‚åœºæ•°æ®..."
            )

            # åˆ›å»ºé€ symbol æ—¥å¿—å›è°ƒå‡½æ•°
            # æ³¨æ„ï¼šä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„åˆ—è¡¨æ”¶é›†æ—¥å¿—ï¼Œå®Œæˆåæ‰¹é‡å†™å…¥
            # é¿å…åœ¨å¤šçº¿ç¨‹å›è°ƒä¸­ç›´æ¥å†™æ•°æ®åº“å¯¼è‡´ SQLite æ­»é”
            from threading import Lock
            fetch_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
            total_symbols = len(symbol_list)
            pending_logs = []  # å¾…å†™å…¥çš„æ—¥å¿—
            log_collect_lock = Lock()  # æ—¥å¿—æ”¶é›†é”
            last_progress_update = [0]  # ä¸Šæ¬¡è¿›åº¦æ›´æ–°æ—¶çš„ count

            def on_symbol_fetched(symbol: str, success: bool, records: int, error_msg: str):
                """æ¯ä¸ª symbol è·å–å®Œæˆåçš„å›è°ƒ - åªæ”¶é›†æ—¥å¿—ï¼Œè°¨æ…æ›´æ–°è¿›åº¦"""
                # åœ¨é”å†…å¿«é€Ÿæ”¶é›†æ—¥å¿—
                with log_collect_lock:
                    fetch_count[0] += 1
                    current_count = fetch_count[0]

                    if success:
                        pending_logs.append({
                            "message": f"âœ“ {symbol} è·å–æˆåŠŸ ({records}æ¡Kçº¿)",
                            "level": "success",
                            "category": "data"
                        })
                    else:
                        # ç²¾ç®€é”™è¯¯ä¿¡æ¯
                        short_error = error_msg[:50] if len(error_msg) > 50 else error_msg
                        pending_logs.append({
                            "message": f"âœ— {symbol} è·å–å¤±è´¥: {short_error}",
                            "level": "warning",
                            "category": "data"
                        })

                # æ¯20ä¸ªæ ‡çš„æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼ˆåœ¨é”å¤–æ‰§è¡Œï¼Œå‡å°‘æŒé”æ—¶é—´ï¼‰
                # ä½¿ç”¨ try/except é˜²æ­¢æ•°æ®åº“é”™è¯¯é˜»å¡ä¸»æµç¨‹
                if current_count % 20 == 0 or current_count == total_symbols:
                    try:
                        progress = 72.0 + (current_count / total_symbols) * 10.0
                        self._update_task_status(
                            task_id,
                            TaskStatus.RUNNING,
                            progress=min(progress, 82.0),
                            step=f"æ­£åœ¨è·å–å¸‚åœºæ•°æ® ({current_count}/{total_symbols})..."
                        )
                    except Exception as e:
                        # è¿›åº¦æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                        logger.warning(f"Progress update failed: {e}")

            # æ‰§è¡Œæ‰¹é‡è·å–ï¼ˆä¼ å…¥å›è°ƒï¼‰
            print(f">>> å¼€å§‹æ‰¹é‡è·å– {len(symbol_list)} ä¸ªæ ‡çš„...")
            stats = fetcher.fetch_required_data(session, progress_callback=on_symbol_fetched)
            print(f">>> æ‰¹é‡è·å–å®Œæˆï¼æˆåŠŸ: {stats.get('symbols_fetched', 0)}, å¤±è´¥: {len(stats.get('failed_symbols', []))}")

            # æ‰¹é‡å†™å…¥æ”¶é›†çš„æ—¥å¿—
            print(f">>> å†™å…¥ {len(pending_logs)} æ¡æ—¥å¿—...")
            for log_entry in pending_logs:
                self._add_log(task_id, log_entry["message"], log_entry["level"], log_entry["category"])

            # è®°å½•æ±‡æ€»ç»“æœ
            self._add_log(task_id, f"åˆ†ææ ‡çš„æ•°: {stats.get('symbols_analyzed', 0)}", "info", "data")
            time.sleep(0.02)
            self._add_log(task_id, f"æˆåŠŸè·å–: {stats.get('symbols_fetched', 0)} ä¸ªæ ‡çš„", "info", "data")
            time.sleep(0.02)
            self._add_log(task_id, f"ç¼“å­˜å‘½ä¸­: {stats.get('cached_symbols', 0)} ä¸ªæ ‡çš„", "info", "data")
            time.sleep(0.02)
            self._add_log(task_id, f"æ•°æ®è®°å½•æ•°: {stats.get('records_fetched', 0)}", "info", "data")
            time.sleep(0.02)

            duration = stats.get('duration_seconds', 0)
            self._add_log(task_id, f"è·å–è€—æ—¶: {duration:.1f} ç§’", "info", "data")

            # æ›´æ–°è¿›åº¦
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=82.0,
                step=f"å·²è·å– {stats.get('symbols_fetched', 0)} ä¸ªæ ‡çš„çš„å¸‚åœºæ•°æ®"
            )

            return stats

        except Exception as e:
            logger.error(f"[{task_id}] Market data fetch error: {e}", exc_info=True)
            self._add_log(task_id, f"âš  å¸‚åœºæ•°æ®è·å–å¼‚å¸¸: {str(e)}", "error", "data")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è¯„åˆ†æµç¨‹ï¼ˆè¿”å›è¯¦ç»†ä¿¡æ¯ä¾›è°ƒç”¨æ–¹åˆ¤æ–­ï¼‰
            return {
                'symbols_fetched': 0,
                'records_fetched': 0,
                'symbols_analyzed': 0,
                'failed_symbols': [],
                'error': str(e)
            }

    def _clear_all_trading_data(self):
        """æ¸…é™¤æ‰€æœ‰äº¤æ˜“æ•°æ®"""
        from sqlalchemy import text

        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        tables_to_clear = ['positions', 'trades', 'import_history']

        try:
            for table in tables_to_clear:
                try:
                    session.execute(text(f"DELETE FROM {table}"))
                    logger.info(f"Cleared table: {table}")
                except Exception as e:
                    logger.warning(f"Failed to clear {table}: {e}")

            session.commit()
        finally:
            session.close()

    def _detect_events_with_logs(self, task_id: str, session) -> int:
        """
        ä¸ºæ–°é…å¯¹çš„æŒä»“æ£€æµ‹å¸‚åœºäº‹ä»¶

        Args:
            task_id: ä»»åŠ¡ID
            session: æ•°æ®åº“ä¼šè¯

        Returns:
            æ£€æµ‹åˆ°çš„äº‹ä»¶æ•°é‡
        """
        from src.analyzers.event_detector import EventDetector
        from src.models.position import Position, PositionStatus

        try:
            self._add_log(task_id, "åˆå§‹åŒ–äº‹ä»¶æ£€æµ‹å™¨...", "info", "events")
            time.sleep(0.03)

            detector = EventDetector(session)

            # è·å–éœ€è¦æ£€æµ‹äº‹ä»¶çš„æŒä»“ï¼ˆå·²å¹³ä»“ä¸”æ²¡æœ‰å…³è”äº‹ä»¶çš„ï¼‰
            positions = session.query(Position).filter(
                Position.status == PositionStatus.CLOSED
            ).all()

            if not positions:
                self._add_log(task_id, "âš  æ²¡æœ‰éœ€è¦æ£€æµ‹äº‹ä»¶çš„æŒä»“", "warning", "events")
                return 0

            self._add_log(task_id, f"å‘ç° {len(positions)} ä¸ªå·²å¹³ä»“æŒä»“éœ€è¦æ£€æµ‹äº‹ä»¶", "info", "events")
            time.sleep(0.03)

            total_events = 0
            symbols_processed = set()

            for i, position in enumerate(positions, 1):
                try:
                    # æ£€æµ‹è¯¥æŒä»“çš„äº‹ä»¶
                    events = detector.detect_events_for_position(
                        position,
                        include_earnings=True,
                        include_anomalies=True
                    )

                    if events:
                        saved = detector.save_events(events, deduplicate=True)
                        total_events += saved

                        if saved > 0:
                            symbols_processed.add(position.symbol)
                            self._add_log(
                                task_id,
                                f"ğŸ“Š {position.symbol}: æ£€æµ‹åˆ° {saved} ä¸ªäº‹ä»¶",
                                "info",
                                "events"
                            )

                    # æ¯10ä¸ªæŒä»“æ›´æ–°ä¸€æ¬¡è¿›åº¦
                    if i % 10 == 0:
                        progress = 90.0 + (i / len(positions)) * 5.0
                        self._update_task_status(
                            task_id,
                            TaskStatus.RUNNING,
                            progress=min(progress, 95.0),
                            step=f"æ­£åœ¨æ£€æµ‹äº‹ä»¶ ({i}/{len(positions)})..."
                        )

                except Exception as e:
                    # å•ä¸ªæŒä»“æ£€æµ‹å¤±è´¥ä¸å½±å“å…¶ä»–
                    logger.warning(f"Event detection failed for position {position.id}: {e}")
                    continue

            # æ±‡æ€»æ—¥å¿—
            self._add_log(
                task_id,
                f"âœ“ äº‹ä»¶æ£€æµ‹å®Œæˆ: {total_events} ä¸ªäº‹ä»¶ï¼Œæ¶‰åŠ {len(symbols_processed)} ä¸ªæ ‡çš„",
                "success",
                "events"
            )

            return total_events

        except Exception as e:
            logger.error(f"[{task_id}] Event detection error: {e}", exc_info=True)
            self._add_log(task_id, f"âš  äº‹ä»¶æ£€æµ‹å¼‚å¸¸: {str(e)}", "warning", "events")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œäº‹ä»¶æ£€æµ‹å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
            return 0

    def _send_completion_email(self, task_id: str, result: dict):
        """å‘é€å®Œæˆé€šçŸ¥é‚®ä»¶"""
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if not task or not task.email:
                return

            # å¯¼å…¥é‚®ä»¶æœåŠ¡å¹¶å‘é€
            try:
                from backend.app.services.email_service import EmailService
                email_service = EmailService()
                email_service.send_analysis_complete(
                    to_email=task.email,
                    task_id=task_id,
                    file_name=task.file_name,
                    result=result
                )
                logger.info(f"[{task_id}] Email notification sent to {task.email}")
            except Exception as e:
                logger.error(f"[{task_id}] Failed to send email: {e}")

        finally:
            session.close()


# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
task_manager = TaskManager()
