"""
ä»»åŠ¡ç®¡ç†å™¨

input: Task æ¨¡å‹, é…ç½®
output: ä»»åŠ¡åˆ›å»ºã€æ‰§è¡Œã€çŠ¶æ€è¿½è¸ª
pos: åç«¯æœåŠ¡å±‚ - ç®¡ç†å¼‚æ­¥åˆ†æä»»åŠ¡çš„æ‰§è¡Œ

åŠŸèƒ½:
- å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ (ThreadPoolExecutor)
- è¯¦ç»†å¤„ç†æ—¥å¿— (æ¯æ¡äº¤æ˜“/æŒä»“/è¯„åˆ†)
- è¿›åº¦è¿½è¸ª (0-100%)
- å®Œæˆé€šçŸ¥ (é‚®ä»¶)

ä¸€æ—¦æˆ‘è¢«æ›´æ–°ï¼ŒåŠ¡å¿…æ›´æ–°æˆ‘çš„å¼€å¤´æ³¨é‡Šï¼Œä»¥åŠæ‰€å±æ–‡ä»¶å¤¹çš„README.md
"""

import sys
import uuid
import logging
import tempfile
import traceback
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable, List
from concurrent.futures import ThreadPoolExecutor
import threading

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import config
from src.models.base import init_database, get_session, create_all_tables
from src.models.task import Task, TaskStatus, TaskType

logger = logging.getLogger(__name__)

# æ—¥å¿—é…ç½®
MAX_LOGS = 1000  # æœ€å¤§æ—¥å¿—æ¡æ•°ï¼ˆå¢åŠ ä»¥å®¹çº³æ›´å¤šæ—¥å¿—ï¼‰
LOG_BATCH_SIZE = 1  # æ¯æ¡éƒ½è®°å½•ï¼Œè®©æ—¥å¿—æ›´é¢‘ç¹
import time  # ç”¨äºæ·»åŠ å°å»¶è¿Ÿè®©åŠ¨ç”»æ›´æ˜æ˜¾


class TaskManager:
    """
    ä»»åŠ¡ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼Œä»»åŠ¡çŠ¶æ€å­˜å‚¨åœ¨ SQLite ä¸­
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        # è·å–é…ç½®
        max_workers = getattr(config, 'TASK_MAX_CONCURRENT', 3)

        self._executor = ThreadPoolExecutor(max_workers=max_workers)
        self._tasks = {}  # task_id -> Future
        self._initialized = True

        logger.info(f"TaskManager initialized with {max_workers} workers")

    def create_task(
        self,
        file_name: str,
        file_hash: str,
        file_size: int,
        file_path: str,
        email: Optional[str] = None,
        replace_mode: bool = True
    ) -> str:
        """
        åˆ›å»ºæ–°ä»»åŠ¡

        Args:
            file_name: æ–‡ä»¶å
            file_hash: æ–‡ä»¶å“ˆå¸Œ
            file_size: æ–‡ä»¶å¤§å°
            file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            email: é€šçŸ¥é‚®ç®±ï¼ˆå¯é€‰ï¼‰
            replace_mode: æ˜¯å¦æ›¿æ¢ç°æœ‰æ•°æ®

        Returns:
            task_id: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())[:8]  # ä½¿ç”¨çŸ­UUID

        # åˆå§‹åŒ–æ•°æ®åº“
        init_database(config.DATABASE_URL, echo=False)
        create_all_tables()

        session = get_session()
        try:
            task = Task(
                task_id=task_id,
                task_type=TaskType.CSV_ANALYSIS,
                status=TaskStatus.PENDING,
                progress=0.0,
                current_step="ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¤„ç†",
                file_name=file_name,
                file_hash=file_hash,
                file_size=file_size,
                email=email,
                logs=[]
            )
            task.add_log("ä»»åŠ¡å·²åˆ›å»º")

            session.add(task)
            session.commit()

            logger.info(f"Task created: {task_id}")

        finally:
            session.close()

        # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        future = self._executor.submit(
            self._run_csv_analysis,
            task_id,
            file_path,
            replace_mode
        )
        self._tasks[task_id] = future

        return task_id

    def get_task(self, task_id: str) -> Optional[dict]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if task:
                return task.to_dict()
            return None
        finally:
            session.close()

    def cancel_task(self, task_id: str) -> bool:
        """
        å–æ¶ˆä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        if task_id in self._tasks:
            future = self._tasks[task_id]
            if future.cancel():
                self._update_task_status(task_id, TaskStatus.CANCELLED)
                logger.info(f"Task cancelled: {task_id}")
                return True

        return False

    def _update_task_status(
        self,
        task_id: str,
        status: str,
        progress: float = None,
        step: str = None,
        result: dict = None,
        error: str = None
    ):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if not task:
                logger.error(f"Task not found: {task_id}")
                return

            task.status = status

            if progress is not None:
                task.progress = progress

            if step:
                task.current_step = step
                task.add_log(step)

            if result:
                task.result = result

            if error:
                task.error_message = error
                task.add_log(error, level="error")

            if status == TaskStatus.RUNNING and not task.started_at:
                task.started_at = datetime.utcnow()

            if status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                task.completed_at = datetime.utcnow()

            session.commit()

        except Exception as e:
            logger.error(f"Failed to update task {task_id}: {e}")
            session.rollback()
        finally:
            session.close()

    def _add_log(
        self,
        task_id: str,
        message: str,
        level: str = "info",
        category: str = None
    ):
        """
        æ·»åŠ æ—¥å¿—æ¡ç›®ï¼ˆä¸æ›´æ–°è¿›åº¦ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ« (info/success/warning/error)
            category: æ—¥å¿—åˆ†ç±» (import/match/score/system)
        """
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if not task:
                return

            log_entry = {
                "time": datetime.utcnow().isoformat(),
                "level": level,
                "message": message,
            }
            if category:
                log_entry["category"] = category

            if task.logs is None:
                task.logs = []

            # é™åˆ¶æ—¥å¿—æ•°é‡
            if len(task.logs) >= MAX_LOGS:
                task.logs = task.logs[-(MAX_LOGS - 1):]

            task.logs = task.logs + [log_entry]  # åˆ›å»ºæ–°åˆ—è¡¨è§¦å‘SQLAlchemyæ›´æ–°
            session.commit()

        except Exception as e:
            logger.error(f"Failed to add log for task {task_id}: {e}")
            session.rollback()
        finally:
            session.close()

    def _run_csv_analysis(self, task_id: str, file_path: str, replace_mode: bool):
        """
        æ‰§è¡Œ CSV åˆ†æä»»åŠ¡ï¼ˆå¸¦è¯¦ç»†æ—¥å¿—ï¼‰

        åˆ†ä¸º4ä¸ªé˜¶æ®µï¼š
        1. æ•°æ®å¯¼å…¥ (0-40%)
        2. FIFOé…å¯¹ (40-70%)
        3. è´¨é‡è¯„åˆ† (70-95%)
        4. å®Œæˆ (95-100%)
        """
        logger.info(f"Starting CSV analysis task: {task_id}")

        self._update_task_status(
            task_id,
            TaskStatus.RUNNING,
            progress=5.0,
            step="å¼€å§‹å¤„ç†æ–‡ä»¶..."
        )
        self._add_log(task_id, "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ", "info", "system")

        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from src.importers.english_csv_parser import detect_csv_language
            from src.importers.incremental_importer import IncrementalImporter
            from src.matchers.fifo_matcher import FIFOMatcher
            from src.analyzers.quality_scorer import QualityScorer
            from src.models.trade import Trade
            from src.models.position import Position
            from sqlalchemy import text

            # ==================== é˜¶æ®µ 1: æ•°æ®å¯¼å…¥ (0-40%) ====================
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=10.0,
                step="æ­£åœ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼..."
            )
            self._add_log(task_id, "æ­£åœ¨æ£€æµ‹ CSV æ–‡ä»¶æ ¼å¼...", "info", "import")

            # æ£€æµ‹è¯­è¨€ - æ·»åŠ æ›´å¤šæ­¥éª¤æ—¥å¿—
            self._add_log(task_id, "è¯»å–æ–‡ä»¶å¤´ä¿¡æ¯...", "info", "import")
            time.sleep(0.1)
            self._add_log(task_id, "åˆ†æå­—æ®µåç§°...", "info", "import")
            time.sleep(0.1)

            language = detect_csv_language(file_path)
            logger.info(f"[{task_id}] Detected language: {language}")

            if language == 'unknown':
                self._add_log(task_id, "æ— æ³•è¯†åˆ«çš„æ–‡ä»¶æ ¼å¼", "error", "import")
                raise ValueError("ä¸æ”¯æŒçš„CSVæ ¼å¼ï¼Œè¯·ä½¿ç”¨å¯Œé€”è¯åˆ¸å¯¼å‡ºçš„CSVæ–‡ä»¶")

            # æ ¼å¼è¯†åˆ«æˆåŠŸ
            format_name = "å¯Œé€”è‹±æ–‡æ ¼å¼" if language == 'english' else "å¯Œé€”ä¸­æ–‡æ ¼å¼"
            self._add_log(task_id, f"âœ“ æ£€æµ‹åˆ°æ ¼å¼: {format_name}", "success", "import")
            time.sleep(0.05)

            # æ›¿æ¢æ¨¡å¼ï¼šå…ˆæ¸…é™¤æ‰€æœ‰æ—§æ•°æ®
            if replace_mode:
                self._update_task_status(
                    task_id,
                    TaskStatus.RUNNING,
                    progress=15.0,
                    step="æ¸…é™¤æ—§æ•°æ®..."
                )
                self._add_log(task_id, "æ­£åœ¨æ¸…é™¤æ—§æ•°æ®...", "info", "import")
                self._add_log(task_id, "æ¸…é™¤äº¤æ˜“è®°å½•è¡¨...", "info", "import")
                time.sleep(0.05)
                self._add_log(task_id, "æ¸…é™¤æŒä»“è®°å½•è¡¨...", "info", "import")
                time.sleep(0.05)
                self._clear_all_trading_data()
                self._add_log(task_id, "æ¸…é™¤å¯¼å…¥å†å²è¡¨...", "info", "import")
                time.sleep(0.05)
                self._add_log(task_id, "âœ“ æ—§æ•°æ®å·²æ¸…é™¤", "success", "import")

            # æ‰§è¡Œå¯¼å…¥
            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=20.0,
                step="æ­£åœ¨å¯¼å…¥äº¤æ˜“æ•°æ®..."
            )
            self._add_log(task_id, "å¼€å§‹è§£æ CSV æ–‡ä»¶...", "info", "import")
            self._add_log(task_id, "åˆå§‹åŒ–è§£æå™¨...", "info", "import")
            time.sleep(0.05)
            self._add_log(task_id, "è¯»å–æ–‡ä»¶å†…å®¹...", "info", "import")
            time.sleep(0.05)

            importer = IncrementalImporter(file_path, dry_run=False)

            self._add_log(task_id, "è§£æ CSV åˆ—ç»“æ„...", "info", "import")
            time.sleep(0.05)
            self._add_log(task_id, "æ˜ å°„å­—æ®µåˆ°æ ‡å‡†æ ¼å¼...", "info", "import")
            time.sleep(0.05)

            import_result = importer.run()

            # æ·»åŠ å¯¼å…¥è¯¦æƒ…æ—¥å¿— - æ›´è¯¦ç»†
            self._add_log(task_id, f"æ–‡ä»¶è§£æå®Œæˆ: å…± {import_result.total_rows} è¡Œæ•°æ®", "info", "import")
            time.sleep(0.03)
            self._add_log(task_id, f"å·²æˆäº¤è®¢å•: {import_result.completed_trades} ç¬”", "info", "import")
            time.sleep(0.03)

            if import_result.duplicates_skipped > 0:
                self._add_log(task_id, f"âš  è·³è¿‡é‡å¤è®°å½•: {import_result.duplicates_skipped} ç¬”", "warning", "import")

            if import_result.errors > 0:
                self._add_log(task_id, f"âš  è§£æé”™è¯¯: {import_result.errors} ç¬”", "warning", "import")
                for err_msg in import_result.error_messages[:5]:
                    self._add_log(task_id, f"  â”” {err_msg}", "warning", "import")
                    time.sleep(0.02)

            # æ·»åŠ å¯¼å…¥çš„æ¯æ¡äº¤æ˜“è¯¦æƒ… - å…¨éƒ¨è®°å½•ï¼
            self._log_all_imported_trades(task_id)

            self._update_task_status(
                task_id,
                TaskStatus.RUNNING,
                progress=40.0,
                step=f"å·²å¯¼å…¥ {import_result.new_trades} æ¡äº¤æ˜“è®°å½•"
            )
            self._add_log(
                task_id,
                f"âœ“ å¯¼å…¥å®Œæˆ: {import_result.new_trades} æ¡æ–°äº¤æ˜“å·²å…¥åº“",
                "success",
                "import"
            )

            # ==================== é˜¶æ®µ 2: FIFOé…å¯¹ (40-70%) ====================
            positions_matched = 0
            positions_scored = 0

            if import_result.new_trades > 0:
                self._update_task_status(
                    task_id,
                    TaskStatus.RUNNING,
                    progress=45.0,
                    step="æ­£åœ¨è¿›è¡ŒæŒä»“é…å¯¹..."
                )
                self._add_log(task_id, "å¼€å§‹ FIFO æŒä»“é…å¯¹ç®—æ³•...", "info", "match")
                time.sleep(0.05)
                self._add_log(task_id, "åˆå§‹åŒ–é…å¯¹å¼•æ“...", "info", "match")
                time.sleep(0.05)
                self._add_log(task_id, "æŒ‰æ ‡çš„åˆ†ç»„äº¤æ˜“è®°å½•...", "info", "match")
                time.sleep(0.05)

                init_database(config.DATABASE_URL, echo=False)
                session = get_session()

                try:
                    matcher = FIFOMatcher(session)

                    self._add_log(task_id, "æ’åºä¹°å…¥/å–å‡ºé˜Ÿåˆ—...", "info", "match")
                    time.sleep(0.05)

                    match_result = matcher.match_all_trades()
                    positions_matched = match_result.get('positions_created', 0)
                    open_positions = match_result.get('open_positions', 0)
                    closed_positions = match_result.get('closed_positions', 0)

                    # æ·»åŠ é…å¯¹è¯¦æƒ…æ—¥å¿—
                    self._add_log(task_id, f"å¤„ç†äº¤æ˜“æ•°: {match_result.get('total_trades', 0)} ç¬”", "info", "match")
                    time.sleep(0.03)
                    self._add_log(task_id, f"æ¶‰åŠæ ‡çš„æ•°: {match_result.get('symbols_processed', 0)} ä¸ª", "info", "match")
                    time.sleep(0.03)

                    # è®°å½•æ¯ä¸ªæŒä»“çš„é…å¯¹ç»“æœ - å…¨éƒ¨ï¼
                    self._log_all_matched_positions(task_id, session)

                    self._add_log(
                        task_id,
                        f"âœ“ é…å¯¹å®Œæˆ: {positions_matched} ä¸ªæŒä»“ (å·²å¹³ä»“: {closed_positions}, æœªå¹³ä»“: {open_positions})",
                        "success",
                        "match"
                    )

                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=70.0,
                        step=f"å·²é…å¯¹ {positions_matched} ä¸ªæŒä»“"
                    )

                    # ==================== é˜¶æ®µ 3: è´¨é‡è¯„åˆ† (70-95%) ====================
                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=75.0,
                        step="æ­£åœ¨è®¡ç®—è´¨é‡è¯„åˆ†..."
                    )
                    self._add_log(task_id, "å¼€å§‹è®¡ç®—è´¨é‡è¯„åˆ† (V2 ä¹ç»´åº¦)...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "åˆå§‹åŒ–è¯„åˆ†å¼•æ“...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "åŠ è½½è¯„åˆ†è§„åˆ™é…ç½®...", "info", "score")
                    time.sleep(0.05)
                    self._add_log(task_id, "è¯„åˆ†ç»´åº¦: æŠ€æœ¯/è¡Œä¸º/é£æ§/æ‰§è¡Œ/å¸‚åœºç¯å¢ƒ...", "info", "score")
                    time.sleep(0.05)

                    scorer = QualityScorer()
                    score_result = scorer.score_all_positions(session, update_db=True)
                    positions_scored = score_result.get('scored', 0)

                    # è®°å½•æ¯ä¸ªæŒä»“çš„è¯„åˆ† - å…¨éƒ¨ï¼
                    self._log_all_scored_positions(task_id, session)

                    session.commit()

                    self._add_log(
                        task_id,
                        f"âœ“ è¯„åˆ†å®Œæˆ: {positions_scored} ä¸ªæŒä»“å·²è¯„åˆ†",
                        "success",
                        "score"
                    )

                    self._update_task_status(
                        task_id,
                        TaskStatus.RUNNING,
                        progress=95.0,
                        step=f"å·²è¯„åˆ† {positions_scored} ä¸ªæŒä»“"
                    )

                except Exception as e:
                    logger.error(f"[{task_id}] Matching/scoring error: {e}")
                    self._add_log(task_id, f"å¤„ç†é”™è¯¯: {str(e)}", "error", "system")
                    session.rollback()
                    raise

                finally:
                    session.close()

            else:
                self._add_log(task_id, "æ— æ–°äº¤æ˜“ï¼Œè·³è¿‡é…å¯¹å’Œè¯„åˆ†", "info", "system")

            # ==================== é˜¶æ®µ 4: å®Œæˆ (100%) ====================
            result = {
                "language": language,
                "total_rows": import_result.total_rows,
                "completed_trades": import_result.completed_trades,
                "new_trades": import_result.new_trades,
                "duplicates_skipped": import_result.duplicates_skipped,
                "positions_matched": positions_matched,
                "positions_scored": positions_scored,
                "errors": import_result.errors,
                "error_messages": import_result.error_messages[:10] if import_result.error_messages else [],
                "broker_name": getattr(import_result, 'broker_name', format_name),
            }

            self._add_log(task_id, "åˆ†æå®Œæˆï¼", "success", "system")
            self._update_task_status(
                task_id,
                TaskStatus.COMPLETED,
                progress=100.0,
                step="åˆ†æå®Œæˆï¼",
                result=result
            )

            logger.info(f"[{task_id}] Task completed successfully")

            # å‘é€é‚®ä»¶é€šçŸ¥
            self._send_completion_email(task_id, result)

        except Exception as e:
            logger.error(f"[{task_id}] Task failed: {e}", exc_info=True)
            self._add_log(task_id, f"ä»»åŠ¡å¤±è´¥: {str(e)}", "error", "system")
            self._update_task_status(
                task_id,
                TaskStatus.FAILED,
                error=str(e)
            )

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(file_path).unlink()
            except:
                pass

    def _log_all_imported_trades(self, task_id: str):
        """è®°å½•æ‰€æœ‰å¯¼å…¥çš„äº¤æ˜“ - æ¯æ¡éƒ½è®°å½•ï¼"""
        from src.models.trade import Trade

        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            # è·å–æ‰€æœ‰äº¤æ˜“ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
            trades = session.query(Trade).order_by(Trade.trade_time.asc()).all()

            if not trades:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(trades)} æ¡äº¤æ˜“æ˜ç»†...", "info", "import")
            time.sleep(0.03)

            for i, trade in enumerate(trades, 1):
                direction = "ä¹°å…¥" if trade.direction == "BUY" else "å–å‡º"
                direction_icon = "ğŸ“ˆ" if trade.direction == "BUY" else "ğŸ“‰"

                # æ ¼å¼åŒ–ä»·æ ¼
                price_str = f"${trade.price:.2f}" if trade.price else "N/A"

                self._add_log(
                    task_id,
                    f"{direction_icon} #{i} {trade.symbol} {direction} {trade.quantity}è‚¡ @ {price_str}",
                    "info",
                    "import"
                )

                # æ¯5æ¡åŠ ä¸€ç‚¹å»¶è¿Ÿï¼Œè®©åŠ¨ç”»æ›´æ˜æ˜¾
                if i % 5 == 0:
                    time.sleep(0.02)

            self._add_log(task_id, f"âœ“ å·²è®°å½• {len(trades)} æ¡äº¤æ˜“æ˜ç»†", "success", "import")

        except Exception as e:
            logger.warning(f"Failed to log trades: {e}")
        finally:
            session.close()

    def _log_all_matched_positions(self, task_id: str, session):
        """è®°å½•æ‰€æœ‰é…å¯¹çš„æŒä»“ - æ¯ä¸ªéƒ½è®°å½•ï¼"""
        from src.models.position import Position

        try:
            # è·å–æ‰€æœ‰æŒä»“
            positions = session.query(Position).order_by(Position.entry_time.asc()).all()

            if not positions:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(positions)} ä¸ªæŒä»“é…å¯¹ç»“æœ...", "info", "match")
            time.sleep(0.03)

            for i, pos in enumerate(positions, 1):
                direction = "å¤šå¤´" if pos.direction == "LONG" else "ç©ºå¤´"
                direction_icon = "ğŸ”º" if pos.direction == "LONG" else "ğŸ”»"
                status = "å·²å¹³ä»“" if pos.status == "CLOSED" else "æŒä»“ä¸­"
                status_icon = "âœ…" if pos.status == "CLOSED" else "â³"

                # ç›ˆäºä¿¡æ¯
                if pos.realized_pnl is not None and pos.realized_pnl != 0:
                    pnl_icon = "ğŸ’°" if pos.realized_pnl > 0 else "ğŸ’¸"
                    pnl_str = f"{pnl_icon} ${pos.realized_pnl:+,.2f}"
                    level = "success" if pos.realized_pnl > 0 else "warning"
                else:
                    pnl_str = ""
                    level = "info"

                self._add_log(
                    task_id,
                    f"{direction_icon} #{i} {pos.symbol} {direction} {pos.quantity}è‚¡ {status_icon}{status} {pnl_str}",
                    level,
                    "match"
                )

                # æ¯3ä¸ªæŒä»“åŠ ä¸€ç‚¹å»¶è¿Ÿ
                if i % 3 == 0:
                    time.sleep(0.02)

            self._add_log(task_id, f"âœ“ å·²è®°å½• {len(positions)} ä¸ªæŒä»“", "success", "match")

        except Exception as e:
            logger.warning(f"Failed to log positions: {e}")

    def _log_all_scored_positions(self, task_id: str, session):
        """è®°å½•æ‰€æœ‰æŒä»“çš„è¯„åˆ† - æ¯ä¸ªéƒ½è®°å½•ï¼"""
        from src.models.position import Position

        try:
            # è·å–æ‰€æœ‰å·²è¯„åˆ†çš„æŒä»“
            positions = session.query(Position).filter(
                Position.quality_score.isnot(None)
            ).order_by(Position.quality_score.desc()).all()

            if not positions:
                return

            self._add_log(task_id, f"æ­£åœ¨è®°å½• {len(positions)} ä¸ªæŒä»“è¯„åˆ†...", "info", "score")
            time.sleep(0.03)

            for i, pos in enumerate(positions, 1):
                grade = pos.quality_grade or "?"
                score = pos.quality_score or 0

                # æ ¹æ®ç­‰çº§é€‰æ‹©å›¾æ ‡
                grade_icons = {
                    'A': 'ğŸ†',
                    'B': 'â­',
                    'C': 'ğŸ“Š',
                    'D': 'âš ï¸',
                    'F': 'âŒ'
                }
                icon = grade_icons.get(grade, 'ğŸ“‹')

                # æ ¹æ®ç­‰çº§é€‰æ‹©æ—¥å¿—çº§åˆ«
                if grade in ['A', 'B']:
                    level = "success"
                elif grade in ['D', 'F']:
                    level = "warning"
                else:
                    level = "info"

                self._add_log(
                    task_id,
                    f"{icon} #{i} {pos.symbol} â†’ {grade}çº§ (åˆ†æ•°: {score:.1f})",
                    level,
                    "score"
                )

                # æ¯3ä¸ªè¯„åˆ†åŠ ä¸€ç‚¹å»¶è¿Ÿ
                if i % 3 == 0:
                    time.sleep(0.02)

            # ç»Ÿè®¡å„ç­‰çº§æ•°é‡
            grade_counts = {}
            for pos in positions:
                g = pos.quality_grade or "?"
                grade_counts[g] = grade_counts.get(g, 0) + 1

            grade_summary = " | ".join([f"{g}çº§:{c}ä¸ª" for g, c in sorted(grade_counts.items())])
            self._add_log(task_id, f"ğŸ“Š è¯„åˆ†åˆ†å¸ƒ: {grade_summary}", "info", "score")

        except Exception as e:
            logger.warning(f"Failed to log scores: {e}")

    def _clear_all_trading_data(self):
        """æ¸…é™¤æ‰€æœ‰äº¤æ˜“æ•°æ®"""
        from sqlalchemy import text

        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        tables_to_clear = ['positions', 'trades', 'import_history']

        try:
            for table in tables_to_clear:
                try:
                    session.execute(text(f"DELETE FROM {table}"))
                    logger.info(f"Cleared table: {table}")
                except Exception as e:
                    logger.warning(f"Failed to clear {table}: {e}")

            session.commit()
        finally:
            session.close()

    def _send_completion_email(self, task_id: str, result: dict):
        """å‘é€å®Œæˆé€šçŸ¥é‚®ä»¶"""
        init_database(config.DATABASE_URL, echo=False)
        session = get_session()

        try:
            task = session.query(Task).filter(Task.task_id == task_id).first()
            if not task or not task.email:
                return

            # å¯¼å…¥é‚®ä»¶æœåŠ¡å¹¶å‘é€
            try:
                from backend.app.services.email_service import EmailService
                email_service = EmailService()
                email_service.send_analysis_complete(
                    to_email=task.email,
                    task_id=task_id,
                    file_name=task.file_name,
                    result=result
                )
                logger.info(f"[{task_id}] Email notification sent to {task.email}")
            except Exception as e:
                logger.error(f"[{task_id}] Failed to send email: {e}")

        finally:
            session.close()


# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
task_manager = TaskManager()
